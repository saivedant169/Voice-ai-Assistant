# Voice-Activated AI Assistant

A sophisticated voice-activated AI assistant that combines speech-to-text, conversational AI, and text-to-speech technologies to provide a natural voice interaction experience.

## Features

- ğŸ¤ **Real-time Speech Recognition** using OpenAI Whisper
- ğŸ§  **Intelligent Conversations** with context awareness
- ğŸ”Š **Natural Text-to-Speech** output
- ğŸ’¬ **Multi-turn Conversations** with memory
- âš¡ **Real-time Processing** for responsive interactions
- ğŸ›ï¸ **Configurable Voice Settings** and wake words

## Tech Stack

- **Speech-to-Text**: OpenAI Whisper
- **Conversational AI**: OpenAI GPT-4 or compatible LLM
- **Text-to-Speech**: pyttsx3 / Azure Speech Services / ElevenLabs
- **Audio Processing**: PyAudio, sounddevice
- **Framework**: Python 3.8+
- **UI**: Streamlit (optional web interface)

## Project Structure

```
voice-ai-assistant/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ speech/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ speech_to_text.py      # Whisper integration
â”‚   â”‚   â””â”€â”€ text_to_speech.py      # TTS implementation
â”‚   â”œâ”€â”€ conversation/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ chat_handler.py        # Conversation management
â”‚   â”‚   â””â”€â”€ memory.py              # Context and memory
â”‚   â”œâ”€â”€ audio/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ recorder.py            # Audio recording
â”‚   â”‚   â””â”€â”€ player.py              # Audio playback
â”‚   â””â”€â”€ assistant.py               # Main assistant class
â”œâ”€â”€ config/
â”‚   â””â”€â”€ settings.py                # Configuration settings
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_speech.py
â”‚   â”œâ”€â”€ test_conversation.py
â”‚   â””â”€â”€ test_assistant.py
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ basic_usage.py
â”‚   â”œâ”€â”€ web_interface.py           # Streamlit web UI
â”‚   â””â”€â”€ cli_assistant.py           # Command line interface
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ setup.py
â”œâ”€â”€ .env.example
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

## Installation

1. Clone this repository
2. Create a virtual environment:
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```
3. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```
4. Copy `.env.example` to `.env` and configure your API keys
5. Run the assistant:
   ```bash
   python examples/cli_assistant.py
   ```

## Configuration

Set up your environment variables in `.env`:
```
OPENAI_API_KEY=your_openai_api_key
AZURE_SPEECH_KEY=your_azure_speech_key (optional)
AZURE_SPEECH_REGION=your_region (optional)
ELEVENLABS_API_KEY=your_elevenlabs_key (optional)
```

## Usage

### Basic Usage
```python
from src.assistant import VoiceAssistant

assistant = VoiceAssistant()
assistant.start_listening()
```

### Web Interface
```bash
streamlit run examples/web_interface.py
```

## Development

### Running Tests
```bash
python -m pytest tests/
```

### Adding New TTS Engines
Extend the `text_to_speech.py` module to support additional TTS services.

### Customizing Wake Words
Modify the `speech_to_text.py` to add custom wake word detection.

## Contributing

1. Fork the repository
2. Create a feature branch
3. Commit your changes
4. Push to the branch
5. Create a Pull Request

## License

MIT License - see LICENSE file for details.

## Acknowledgments

- OpenAI for Whisper and GPT models
- Contributors to open-source speech processing libraries
